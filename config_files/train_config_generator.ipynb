{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93063100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "dualview_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a1f207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config={\n",
    "                   'base_epoch': 0,\n",
    "                   'batch_size': 64,\n",
    "                   'epochs': 200,\n",
    "                   'save_each': 10,\n",
    "                   'num_batches_eval': 200,\n",
    "                   'validation_size': 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab30a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_local(config, config_name):\n",
    "    config['device'] = 'cpu'\n",
    "    config['data_root'] = f'{dualview_path}/src/datasets'\n",
    "    config['save_dir'] = f'{dualview_path}/checkpoints/{config['dataset_name']}/{config['dataset_type']}/{config['model_name']}_{config['dataset_type']}'\n",
    "    config['epochs'] = 2\n",
    "    config['save_each'] = 1\n",
    "    path = f'local/train/{config['dataset_name']}'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    with open(f'{path}/{config_name}.yaml', 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False)\n",
    "        \n",
    "def create_config_cluster(config, config_name):\n",
    "    config['device']='cuda'\n",
    "    config['data_root'] = '/mnt/dataset/'\n",
    "    config['save_dir'] = '/mnt/outputs/'\n",
    "    config['epochs'] = 200\n",
    "    config['save_each'] = 10\n",
    "    \n",
    "    path = f'cluster/train/{config['dataset_name']}'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    with open(f'{path}/{config_name}.yaml', 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64169599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsname_list = ['MNIST', 'CIFAR', 'AWA']\n",
    "dstype_list = ['std'] #['std', 'group', 'corrupt', 'mark']\n",
    "lr_list = [1e-5, 1e-4] #[1e-5, 5e-5, 1e-4, 5e-4]\n",
    "momentum_list = [0, 0.9] #[0, 0.9, 0.99]\n",
    "weight_decay_list = [0, 1e-4, 1e-2] #[0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "optimizer_list = ['sgd', 'adam', 'rmsprop']\n",
    "scheduler_list = ['constant'] #['constant', 'step', 'annealing']\n",
    "loss_list = ['cross_entropy', 'hinge']\n",
    "augmentation_list = [(a + b + c + d)[:-1] for a in ['crop_', ''] for b in ['flip_', ''] for c in ['eq_', ''] for d in ['rotate_', '']] #'cifar', 'imagenet'\n",
    "\n",
    "model_dict = {'MNIST': 'basic_conv', 'CIFAR': 'resnet18', 'AWA': 'resnet50'} #remove this\n",
    "num_classes_dict = {'MNIST': 10, 'CIFAR': 10, 'AWA': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63c956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating config files for dataset MNIST...\n",
      "Creating config files for dataset CIFAR...\n",
      "Creating config files for dataset AWA...\n"
     ]
    }
   ],
   "source": [
    "for dsname in dsname_list:\n",
    "    base_config['dataset_name'] = dsname\n",
    "    base_config['model_name'] = model_dict[dsname]\n",
    "    base_config['num_classes'] = num_classes_dict[dsname]\n",
    "    base_config['class_groups'] = [[2*i,2*i+1] for i in range(base_config['num_classes'] // 2)]\n",
    "    print(f'Creating config files for dataset {dsname}...')\n",
    "\n",
    "    for dstype in dstype_list:\n",
    "        base_config['dataset_type'] = dstype\n",
    "\n",
    "        for lr in lr_list:\n",
    "            base_config['lr'] = lr\n",
    "\n",
    "            for momentum in momentum_list:\n",
    "                base_config['momentum'] = momentum\n",
    "\n",
    "                for weight_decay in weight_decay_list:\n",
    "                    base_config['weight_decay'] = weight_decay\n",
    "\n",
    "                    for optimizer in optimizer_list:\n",
    "                        base_config['optimizer'] = optimizer\n",
    "\n",
    "                        for scheduler in scheduler_list:\n",
    "                            base_config['scheduler'] = scheduler\n",
    "\n",
    "                            for loss in loss_list:\n",
    "                                base_config['loss'] = loss\n",
    "\n",
    "                                for augmentation in augmentation_list:\n",
    "                                    if augmentation != '':\n",
    "                                        base_config['augmentation'] = augmentation\n",
    "\n",
    "                                    config_filename = f'{dsname}_{dstype}_{lr}_{momentum}_{weight_decay}_{optimizer}_{scheduler}_{loss}_{augmentation}'\n",
    "                                    create_config_cluster(base_config, config_filename)\n",
    "                                    create_config_local(base_config, config_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
