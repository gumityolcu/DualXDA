{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('C:/Users/weckbecker/DualView-wip/')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from random import sample as samp\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard dataset\n",
    "sample = torch.load('explanations/MNIST/std/basic_conv_std/dualview_0.001/samples_tensor', map_location=torch.device('cpu'))\n",
    "label = torch.load('explanations/MNIST/std/basic_conv_std/dualview_0.001/labels_tensor', map_location=torch.device('cpu'))\n",
    "\n",
    "# corrupted dataset\n",
    "#sample = torch.load('explanations/MNIST/corrupt/basic_conv_corrupt/dualview_0.001/samples_tensor')\n",
    "#label = torch.load('explanations/MNIST/corrupt/basic_conv_corrupt/dualview_0.001/labels_tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 100]), torch.Size([60000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take entire data as training data\n",
    "\n",
    "train_size = int(1* len(sample))\n",
    "test_size = len(sample) - train_size\n",
    "\n",
    "sample_train = sample[:train_size]\n",
    "sample_test = sample[train_size:]\n",
    "label_train = label[:train_size]\n",
    "label_test = label[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR(penalty='l2', tol=0.0000001, C=1.0, max_iter=1000)\n",
    "lr.fit(sample_train, label_train)\n",
    "lr_w = lr.coef_\n",
    "\n",
    "svc = SVC(penalty = 'l2', tol=0.0000001, C=1.0, max_iter=10000, dual=False)\n",
    "svc.fit(sample_train, label_train)\n",
    "svc_w = svc.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicConvModel(\n",
       "  (features): Sequential(\n",
       "    (conv-0): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu-0): ReLU()\n",
       "    (conv-1): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu-1): ReLU()\n",
       "    (conv-2): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu-2): ReLU()\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc-0): Linear(in_features=2420, out_features=500, bias=True)\n",
       "    (relu-3): ReLU()\n",
       "    (fc-1): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (relu-4): ReLU()\n",
       "  )\n",
       "  (classifier): Linear(in_features=100, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.models import load_model\n",
    "\n",
    "model_name = 'basic_conv'\n",
    "dataset_name = 'MNIST'\n",
    "num_classes = 10\n",
    "model_path = 'C:/Users/weckbecker/DualView/checkpoints/MNIST/std/basic_conv_std/MNIST_basic_conv'\n",
    "device = 'cpu'\n",
    "\n",
    "model = load_model(model_name, dataset_name, num_classes).to(device)\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7107294380608492\n",
      "0.850249240329069\n",
      "0.8049249654039301\n"
     ]
    }
   ],
   "source": [
    "cnn_w = model.classifier.weight.detach().numpy()\n",
    "print(np.average(np.diag(cosine_similarity(cnn_w, svc_w))))\n",
    "print(np.average(np.diag(cosine_similarity(cnn_w, lr_w))))\n",
    "print(np.average(np.diag(cosine_similarity(lr_w, svc_w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.no_grad()\n",
    "\n",
    "\n",
    "trainset=torchvision.datasets.MNIST('C:/Users/weckbecker/DualView/src/datasets',\n",
    "                                    transform = transforms.Compose([\n",
    "                                    transforms.ToTensor()\n",
    "                                    ]))\n",
    "train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                           batch_size=60000,\n",
    "                                           shuffle=False)\n",
    "data, labels = next(iter(train_loader))\n",
    "logits = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# closed form solution for regression is inv(X.T @ X) @ X.T @ y\n",
    "# implementation scikit learn also exists\n",
    "\n",
    "logits_train = logits.detach().numpy()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(sample_train, logits_train)\n",
    "linreg_w = lin_reg.coef_\n",
    "linreg_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8021429\n",
      "0.6237550319703999\n",
      "0.7496541667140237\n"
     ]
    }
   ],
   "source": [
    "print(np.average(np.diag(cosine_similarity(cnn_w, linreg_w))))\n",
    "print(np.average(np.diag(cosine_similarity(svc_w, linreg_w))))\n",
    "print(np.average(np.diag(cosine_similarity(lr_w, linreg_w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8021429\n",
      "0.7107294380608492\n",
      "0.850249240329069\n"
     ]
    }
   ],
   "source": [
    "print(np.average(np.diag(cosine_similarity(linreg_w, cnn_w))))\n",
    "print(np.average(np.diag(cosine_similarity(svc_w, cnn_w))))\n",
    "print(np.average(np.diag(cosine_similarity(lr_w, cnn_w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last 1000 datapoints\n",
    "\n",
    "sample_train_cut = sample_train[:59000,:]\n",
    "label_train_cut = label_train[:59000]\n",
    "logits_train_cut = logits_train[:59000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_2 = LR(penalty='l2', tol=0.0000001, C=1.0, max_iter=1000)\n",
    "lr_2.fit(sample_train_cut, label_train_cut)\n",
    "lr_w_2 = lr_2.coef_\n",
    "\n",
    "svc_2 = SVC(penalty = 'l2', tol=0.0000001, C=1.0, max_iter=10000, dual=False)\n",
    "svc_2.fit(sample_train_cut, label_train_cut)\n",
    "svc_w_2 = svc_2.coef_\n",
    "\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(sample_train_cut, logits_train_cut)\n",
    "linreg_w_2 = lin_reg_2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8049249654039301\n",
      "0.6237550319703999\n",
      "0.7496541667140237\n",
      "\n",
      "0.801581701103179\n",
      "0.6264896380054895\n",
      "0.7515252965494627\n"
     ]
    }
   ],
   "source": [
    "print(np.average(np.diag(cosine_similarity(svc_w, lr_w))))\n",
    "print(np.average(np.diag(cosine_similarity(svc_w, linreg_w))))\n",
    "print(np.average(np.diag(cosine_similarity(lr_w, linreg_w))))\n",
    "print('')\n",
    "print(np.average(np.diag(cosine_similarity(svc_w_2, lr_w_2))))\n",
    "print(np.average(np.diag(cosine_similarity(svc_w_2, linreg_w_2))))\n",
    "print(np.average(np.diag(cosine_similarity(lr_w_2, linreg_w_2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What regulatisation stays more faithful to weight vector of perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\weckbecker\\AppData\\Local\\Temp\\ipykernel_45768\\2250321556.py:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(f'Logistic regression, C = {10**power}\\n\\ Cosine similarity: {np.average(np.diag(cosine_similarity(lr_w, cnn_w)))}')\n",
      "C:\\Users\\weckbecker\\AppData\\Local\\Temp\\ipykernel_45768\\2250321556.py:11: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(f'SVC, C = {10**power}\\n\\ Cosine similarity: {np.average(np.diag(cosine_similarity(svc_w, cnn_w)))}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, C = 1e-06\n",
      "\\ Cosine similarity: 0.581425150057394\n",
      "Logistic regression, C = 1e-05\n",
      "\\ Cosine similarity: 0.6129486973707714\n",
      "Logistic regression, C = 0.0001\n",
      "\\ Cosine similarity: 0.6938531577662418\n",
      "Logistic regression, C = 0.001\n",
      "\\ Cosine similarity: 0.7940323525656501\n",
      "Logistic regression, C = 0.01\n",
      "\\ Cosine similarity: 0.8759043531233697\n",
      "Logistic regression, C = 0.1\n",
      "\\ Cosine similarity: 0.9005550148086428\n",
      "Logistic regression, C = 1\n",
      "\\ Cosine similarity: 0.8473164223742602\n",
      "Logistic regression, C = 10\n",
      "\\ Cosine similarity: 0.7617532344357535\n",
      "Logistic regression, C = 100\n",
      "\\ Cosine similarity: 0.69447742643438\n",
      "Logistic regression, C = 1000\n",
      "\\ Cosine similarity: 0.671170134718768\n",
      "SVC, C = 1e-06\n",
      "\\ Cosine similarity: 0.6246915479825653\n",
      "SVC, C = 1e-05\n",
      "\\ Cosine similarity: 0.6464378787129859\n",
      "SVC, C = 0.0001\n",
      "\\ Cosine similarity: 0.6443789172761785\n",
      "SVC, C = 0.001\n",
      "\\ Cosine similarity: 0.6937400559517685\n",
      "SVC, C = 0.01\n",
      "\\ Cosine similarity: 0.7222426014456499\n",
      "SVC, C = 0.1\n",
      "\\ Cosine similarity: 0.7214304537267662\n",
      "SVC, C = 1\n",
      "\\ Cosine similarity: 0.71074295269215\n",
      "SVC, C = 10\n",
      "\\ Cosine similarity: 0.7045933820955058\n",
      "SVC, C = 100\n",
      "\\ Cosine similarity: 0.703343523964307\n",
      "SVC, C = 1000\n",
      "\\ Cosine similarity: 0.7031774930812004\n"
     ]
    }
   ],
   "source": [
    "for power in range(-6,4,1):\n",
    "    lr = LR(penalty='l2', tol=0.00001, C=10**power, max_iter=1000)\n",
    "    lr.fit(sample_train, label_train)\n",
    "    lr_w = lr.coef_\n",
    "    print(f'Logistic regression, C = {10**power}\\n\\ Cosine similarity: {np.average(np.diag(cosine_similarity(lr_w, cnn_w)))}')\n",
    "\n",
    "for power in range(-6,4,1):\n",
    "    svc = SVC(penalty = 'l2', tol=0.00001, C=10**power, max_iter=10000, dual=False)\n",
    "    svc.fit(sample_train, label_train)\n",
    "    svc_w = svc.coef_\n",
    "    print(f'SVC, C = {10**power}\\n\\ Cosine similarity: {np.average(np.diag(cosine_similarity(svc_w, cnn_w)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Leverage computation\n",
    "\n",
    "Taking $X \\in \\mathbb{R}^{n \\times p}$ as the matrix of activations in the penultimate layer, we calculate the hat matrix of logistic regression, given by $X(X^TX)^{-1}X^T$.\n",
    "For a test datapoint with activation $x_{test}$, we define the extended matrix $X_+ \\in \\mathbb{R}^{(n+1) \\times p}$. We can then use the Sherman-Morrison formula to calculate the needed inverse. Then the leverage of datapoint $i$ on the test datapoint is indeed proportional to $x_i^T(X^TX)^{-1}x_{test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 100])\n",
      "torch.Size([100, 100])\n"
     ]
    }
   ],
   "source": [
    "X = sample\n",
    "print(X.shape)\n",
    "inv = torch.inverse(X.T @ X)\n",
    "print(inv.shape)\n",
    "H = X @ inv @ X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0025)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0] @ inv @ X[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0025)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 60000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dualview_xpl = torch.load('explanations/MNIST/std/basic_conv_std/dualview_0.001/DualViewExplainer_0', map_location=torch.device('cpu'))\n",
    "dualview_xpl.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
